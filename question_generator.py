# question_generator.py
# ------------------------------------------------------------
# CV-based question generation using Google Gemini.
#
# Design:
#   - Total 5 questions:
#       Q1, Q2: fixed warm-up questions
#       Q3, Q4, Q5: generated by LLM from CV
#   - Each question has:
#       - difficulty: "easy" | "medium" | "hard"
#       - expected_time_s: expected answer time in seconds
#       - followups: a list of short follow-up questions (for Q3–Q5)
#
# Follow-up rule (used by the controller, not by this file):
#   - For each of Q3/Q4/Q5:
#       * Let expected_time_s be the expected answer time.
#       * Measure actual answer duration (actual_time_s).
#       * If:
#             0.5 * expected_time_s < actual_time_s < expected_time_s
#         and no follow-up has been used before:
#             -> ask ONE follow-up question for this main question.
#   - At most ONE follow-up in the whole interview:
#       If Q3 used a follow-up, then Q4 and Q5 cannot trigger follow-ups.
#
#   - This module only provides the question content + difficulty + expected_time_s + followups.

from __future__ import annotations

import json
import os
from dataclasses import dataclass, field
from typing import Dict, List, Any

from dotenv import load_dotenv
from google import genai

load_dotenv()


# ------------------------------------------------------------
# Data structure for a question
# ------------------------------------------------------------

@dataclass
class InterviewQuestion:
    id: str
    text: str
    difficulty: str            # "easy" | "medium" | "hard"
    expected_time_s: float     # expected answer time in seconds
    category: str = "general"
    source: str = "cv_based"
    followups: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "text": self.text,
            "difficulty": self.difficulty,
            "expected_time_s": self.expected_time_s,
            "category": self.category,
            "source": self.source,
            "followups": self.followups,
        }


# ------------------------------------------------------------
# Question generator using Gemini
# ------------------------------------------------------------

class QuestionGenerator:
    """
    Generate interview questions from candidate CV data using Google Gemini.
    The API format is aligned with Person 4's CVParser (google.genai).
    """

    def __init__(self):
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY not set in .env")

        self.client = genai.Client(api_key=api_key)
        self.model_name = "gemini-2.0-flash"

    # ---------- fixed warm-up questions (Q1, Q2) ----------

    @staticmethod
    def build_fixed_questions() -> List[InterviewQuestion]:
        """
        Two fixed warm-up questions with expected answer times.

        You can tune these durations:
          - Q1: very short, simple check-in
          - Q2: slightly longer self-introduction
        """
        q1 = InterviewQuestion(
            id="Q1",
            text="How have you been feeling lately?",
            difficulty="easy",
            expected_time_s=15.0,
            category="warmup",
            source="fixed",
        )
        q2 = InterviewQuestion(
            id="Q2",
            text="Ok, thanks. Could you please introduce yourself?",
            difficulty="easy",
            expected_time_s=40.0,
            category="warmup",
            source="fixed",
        )
        return [q1, q2]

    # ---------- LLM prompt ----------

    def _build_prompt_for_cv_questions(self, cv_data: Dict[str, Any], position: str) -> str:
        """
        Build a prompt that uses structured CV data from CVParser.
        cv_data example:
            {
              "name": "...",
              "email": "...",
              "education": [...],
              "experience": [...],
              "skills": [...]
            }
        """
        cv_json = json.dumps(cv_data, ensure_ascii=False, indent=2)

        prompt = (
            "You are an interview assistant for a social robot.\n"
            "The robot is interviewing a candidate for the position: "
            f"\"{position}\".\n\n"
            "You are given the candidate's CV in a structured JSON format.\n"
            "Based on this CV, generate exactly 3 follow-up interview questions\n"
            "that we will label as Q3, Q4, and Q5.\n\n"
            "Requirements for the MAIN questions:\n"
            "- Each question must be directly related to the candidate's CV\n"
            "  (education, experience, projects, or skills).\n"
            "- Each question must have a difficulty level:\n"
            "    * easy: simple, high-level questions, low pressure.\n"
            "    * medium: more specific questions about responsibilities or skills.\n"
            "    * hard: deeper questions about methods, design decisions, or failures.\n"
            "- Mix the difficulties (not all the same).\n\n"
            "FOLLOW-UP REQUIREMENT:\n"
            "- For EACH main question, also generate 1–2 short follow-up questions.\n"
            "- Follow-ups should:\n"
            "    * ask for examples, clarification, or more detail,\n"
            "    * still relate to the same topic as the main question,\n"
            "    * remain in a natural interview style.\n\n"
            "Output format (VERY IMPORTANT):\n"
            "- Return ONLY a valid JSON array (no markdown, no ``` fences, no comments).\n"
            "- The array must contain exactly 3 objects.\n"
            "- Each object must have exactly three fields:\n"
            "    * \"text\"       : string, the main question.\n"
            "    * \"difficulty\" : one of \"easy\", \"medium\", \"hard\".\n"
            "    * \"followups\"  : a JSON array of 1–2 short follow-up questions (strings).\n\n"
            "Example of the correct format:\n"
            "[\n"
            "  {\n"
            "    \"text\": \"High-level question?\",\n"
            "    \"difficulty\": \"easy\",\n"
            "    \"followups\": [\n"
            "      \"Can you give a concrete example?\",\n"
            "      \"What did you learn from that experience?\"\n"
            "    ]\n"
            "  },\n"
            "  {\n"
            "    \"text\": \"More detailed question?\",\n"
            "    \"difficulty\": \"medium\",\n"
            "    \"followups\": [\n"
            "      \"Which part was most challenging?\"\n"
            "    ]\n"
            "  },\n"
            "  {\n"
            "    \"text\": \"Deep technical question?\",\n"
            "    \"difficulty\": \"hard\",\n"
            "    \"followups\": [\n"
            "      \"Why did you choose that approach?\"\n"
            "    ]\n"
            "  }\n"
            "]\n\n"
            "Now here is the candidate CV data:\n"
            f"{cv_json}\n\n"
            "Generate the JSON array now."
        )
        return prompt

    # ---------- Gemini call + JSON parsing ----------

    @staticmethod
    def _strip_code_fence(raw: str) -> str:
        """
        Remove ``` or ```json fences if Gemini wraps the JSON.
        """
        text = raw.strip()
        if text.startswith("```"):
            text = text.strip("`").strip()
            if text.lower().startswith("json"):
                text = text[4:].strip()
        return text

    def _call_gemini_for_questions(self, prompt: str) -> List[Dict[str, Any]]:
        """
        Call Gemini and return a list[dict] parsed from JSON.
        Each dict should contain: "text", "difficulty", "followups".
        """
        response = self.client.models.generate_content(
            model=self.model_name,
            contents=prompt,
        )
        raw = response.text or ""
        raw = self._strip_code_fence(raw)

        # Optional debug
        with open("gemini_question_raw_debug.txt", "w", encoding="utf-8") as dbg:
            dbg.write(raw)

        data = json.loads(raw)
        if not isinstance(data, list):
            raise ValueError("Gemini output is not a JSON array.")

        return data

    # ---------- difficulty → expected time mapping ----------

    @staticmethod
    def _expected_time_for_difficulty(difficulty: str) -> float:
        """
        Map difficulty level to an expected answer time (seconds).
        You can adjust the values if needed.
        """
        diff = difficulty.lower().strip()
        if diff == "easy":
            return 25.0  # short, low pressure
        if diff == "medium":
            return 45.0  # more detailed answer
        if diff == "hard":
            return 70.0  # deeper technical/reflection
        return 40.0

    # ---------- public API ----------

    def generate_full_question_set(
        self,
        cv_data: Dict[str, Any],
        position: str,
    ) -> List[Dict[str, Any]]:
        """
        Generate the complete set of 5 questions:
          - Q1, Q2: fixed warm-up
          - Q3, Q4, Q5: Gemini-based from CV (with followups)

        Returns:
            List of 5 question dicts, each containing:
              {
                "id": "Q1".."Q5",
                "text": "...",
                "difficulty": "...",
                "expected_time_s": float,
                "category": "...",
                "source": "fixed" or "cv_based",
                "followups": [ ... ]   # empty for Q1/Q2, filled for Q3–Q5
              }
        """
        questions: List[InterviewQuestion] = []

        # 1) fixed warmup
        warmup = self.build_fixed_questions()
        questions.extend(warmup)

        # 2) LLM-generated (3 questions -> Q3, Q4, Q5)
        prompt = self._build_prompt_for_cv_questions(cv_data, position)
        raw_items = self._call_gemini_for_questions(prompt)

        # We only need 3 questions; if Gemini returns more, ignore the rest.
        max_llm_q = 3
        raw_items = raw_items[:max_llm_q]

        next_id = 3
        for item in raw_items:
            if not isinstance(item, dict):
                continue

            text = str(item.get("text", "")).strip()
            diff = str(item.get("difficulty", "")).strip().lower()
            followups_raw = item.get("followups", [])

            if not text:
                continue
            if diff not in {"easy", "medium", "hard"}:
                diff = "medium"

            # ensure followups is a list of strings
            followup_list: List[str] = []
            if isinstance(followups_raw, list):
                for f in followups_raw:
                    s = str(f).strip()
                    if s:
                        followup_list.append(s)

            expected_time = self._expected_time_for_difficulty(diff)

            q = InterviewQuestion(
                id=f"Q{next_id}",
                text=text,
                difficulty=diff,
                expected_time_s=expected_time,
                category="cv_based",
                source="cv_based",
                followups=followup_list,
            )
            questions.append(q)
            next_id += 1

        return [q.to_dict() for q in questions]
