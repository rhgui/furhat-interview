# question_generator.py
# ------------------------------------------------------------
# CV-based question generation using Google Gemini.
#
# New design (2025-12):
#   - Total 5 questions:
#       Q1, Q2: fixed warm-up questions (greeting / self-intro)
#       Q3, Q4, Q5: generated by LLM from CV
#
#   - For each CV question (Q3–Q5) we need:
#       * original_main      : the normal version of the main question
#       * guided_main        : a more scaffolded / gentle version
#       * expected_time_s    : expected answering time in seconds
#
#   - Difficulty:
#       * All generated questions are tagged as "medium" (uniform).
#
#   - The higher-level controller will use these fields to implement:
#       * Which version to ask (original_main vs guided_main)
#       * How long an answer is expected to be (expected_time_s)
# ------------------------------------------------------------

from __future__ import annotations

import json
import os
from dataclasses import dataclass, field
from typing import Dict, List, Any

from dotenv import load_dotenv
from openai import OpenAI
load_dotenv()


# ------------------------------------------------------------
# Data structure for a question
# ------------------------------------------------------------

@dataclass
class InterviewQuestion:
    # Generic fields used by the rest of the system
    id: str
    text: str                  # default main question = original_main
    difficulty: str            # always "medium" for CV questions
    expected_time_s: float     # expected answer time in seconds
    category: str = "general"
    source: str = "cv_based"

    # Extra fields for new design
    guided_main: str = ""          # guided version of the main question


    def to_dict(self) -> Dict[str, Any]:
        """
        Convert to a plain dict for JSON / downstream modules.
        """
        return {
            "id": self.id,
            "text": self.text,
            "difficulty": self.difficulty,
            "expected_time_s": self.expected_time_s,
            "category": self.category,
            "source": self.source,
            "guided_main": self.guided_main,
        }


# ------------------------------------------------------------
# Question generator using Gemini
# ------------------------------------------------------------

class QuestionGenerator:
    """
    Generate interview questions from candidate CV data using DeepSeek API.
    """

    def __init__(self):
        api_key = os.getenv("DEEPSEEK_API_KEY")
        if not api_key:
            raise ValueError("DEEPSEEK_API_KEY not set in .env")

        # DeepSeek 使用 OpenAI 兼容接口
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com"
        )
        # 默认用 deepseek-chat，你也可以在 .env 里配置别的
        self.model_name = os.getenv("DEEPSEEK_MODEL_NAME", "deepseek-chat")

    # ---------- fixed warm-up questions (Q1, Q2) ----------

    @staticmethod
    def build_fixed_questions() -> List[InterviewQuestion]:
        """
        Two fixed warm-up questions with expected answer times.

        You can tune these durations:
          - Q1: very short, simple check-in
          - Q2: slightly longer self-introduction
        """
        q1 = InterviewQuestion(
            id="Q1",
            text="How have you been feeling lately?",
            difficulty="medium",
            expected_time_s=15.0,
            category="warmup",
            source="fixed",
        )
        q2 = InterviewQuestion(
            id="Q2",
            text="Ok, thanks. Could you please introduce yourself?",
            difficulty="medium",
            expected_time_s=40.0,
            category="warmup",
            source="fixed",
        )
        return [q1, q2]

    # ---------- LLM prompt ----------

    def _build_prompt_for_cv_questions(self, cv_data: Dict[str, Any], position: str) -> str:
        """
        Build a prompt that uses structured CV data from CVParser.
        We ask Gemini to generate for EACH of Q3/Q4/Q5:
          - original_main
          - guided_main
          - expected_time_s (seconds, numeric)
        All questions should be 'medium' difficulty – we will set that
        ourselves in the code.
        """
        cv_json = json.dumps(cv_data, ensure_ascii=False, indent=2)

        prompt = (
            "You are an HR interviewer working with a social robot.\n"
            "The robot is interviewing a candidate for the position: "
            f"\"{position}\".\n\n"
            "You are given the candidate's CV in a structured JSON format.\n"
            "Based on this CV, generate EXACTLY 3 interview questions that\n"
            "we will label as Q3, Q4, and Q5.\n\n"
            "For EACH question, you must provide:\n"
            "  - \"original_main\"     : the normal version of the question, in a\n"
            "                           professional, neutral style.\n"
            "  - \"guided_main\"       : a more scaffolded and gentle version of\n"
            "                           the SAME question, designed to reduce\n"
            "                           anxiety (e.g., more structure, hints,\n"
            "                           breaking the question into smaller pieces).\n"
            "  - \"expected_time_s\"   : a reasonable expected answer time in\n"
            "                           SECONDS (number, no quotes). Think about\n"
            "                           how long a solid but not overly long\n"
            "                           answer should take.\n"
            "Requirements for the content:\n"
            "- All questions must be directly related to the candidate's CV\n"
            "  (education, experience, projects, or skills).\n"
            "- Assume the difficulty of all main questions is 'medium': ask\n"
            "  for specific responsibilities, tools, or decisions, but avoid\n            "
            " extremely hard, exam-style questions.\n"
            "Output format (VERY IMPORTANT):\n"
            "- Return ONLY a valid JSON array (no markdown, no ``` fences,\n"
            "  no comments, no extra text).\n"
            "- The array MUST contain exactly 3 objects.\n"
            "- Each object must have EXACTLY these three fields:\n"
            "    * \"original_main\"     : string\n"
            "    * \"guided_main\"       : string\n"
            "    * \"expected_time_s\"   : number (seconds)\n"
            "Example of the correct format (the content is just an example):\n"
            "[\n"
            "  {\n"
            "    \"original_main\": \"Can you describe a project where you used Python for data analysis?\",\n"
            "    \"guided_main\":   \"You mentioned using Python for data analysis. Could you briefly walk me through one project: what was the goal, what data you used, and how you processed it?\",\n"
            "    \"expected_time_s\": 50,\n"
            "  },\n"
            "  {\n"
            "    \"original_main\": \"How have you applied your robotics knowledge in practical projects?\",\n"
            "    \"guided_main\":   \"Thinking about your robotics background, could you pick one project and explain the problem, your role, and the key technical decisions you made?\",\n"
            "    \"expected_time_s\": 55,\n"
            "  },\n"
            "  {\n"
            "    \"original_main\": \"Can you tell me about a situation where you worked in a cross-functional team?\",\n"
            "    \"guided_main\":   \"Please recall a time when you worked with people from different backgrounds (for example, hardware, software, or business). What was the goal, what was your role, and how did you collaborate?\",\n"
            "    \"expected_time_s\": 45,\n"
            "  }\n"
            "]\n\n"
            "Now here is the candidate CV data:\n"
            f"{cv_json}\n\n"
            "Generate ONLY the JSON array now."
        )
        return prompt

    # ---------- Gemini call + JSON parsing ----------

    @staticmethod
    def _strip_code_fence(raw: str) -> str:
        """
        Remove ``` or ```json fences if Gemini wraps the JSON.
        """
        text = raw.strip()
        if text.startswith("```"):
            text = text.strip("`").strip()
            if text.lower().startswith("json"):
                text = text[4:].strip()
        return text

    def _call_gemini_for_questions(self, prompt: str) -> str:
        """
        调用 DeepSeek 模型，返回 LLM 的原始文本。
        函数名保留 _call_gemini_for_questions，避免改动其它代码。
        """
        resp = self.client.chat.completions.create(
            model=self.model_name,          # 如 deepseek-chat
            messages=[
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
            temperature=0.7,
        )

        # 1) 拿到原始文本
        raw_text = resp.choices[0].message.content

        # 2) 去掉 ```json ``` 这种 code fence
        cleaned = self._strip_code_fence(raw_text)

        # 3) 解析 JSON
        try:
            data = json.loads(cleaned)
        except json.JSONDecodeError as e:
            print("⚠ DeepSeek 返回内容不是合法 JSON，错误：", e)
            print("⚠ 原始文本：", cleaned[:400], "...")
            return []

        # 4) 保证是 list
        if not isinstance(data, list):
            print("⚠ 解析结果不是 list，实际类型：", type(data))
            return []

        return data

    # ---------- public API ----------

    def generate_full_question_set(
        self,
        cv_data: Dict[str, Any],
        position: str,
    ) -> List[Dict[str, Any]]:
        """
        Generate the complete set of 5 questions:
          - Q1, Q2: fixed warm-up
          - Q3, Q4, Q5: Gemini-based from CV

        Returns:
            List of 5 question dicts, each containing at least:
              {
                "id": "Q1".."Q5",
                "text": "...",                 # main question (original_main)
                "difficulty": "easy|medium",
                "expected_time_s": float,
                "category": "...",
                "source": "fixed" or "cv_based",

                # new fields for Q3–Q5:
                "guided_main": "...",

              }
        """
        questions: List[InterviewQuestion] = []

        # 1) fixed warmup
        warmup = self.build_fixed_questions()
        questions.extend(warmup)

        # 2) LLM-generated (3 questions -> Q3, Q4, Q5)
        prompt = self._build_prompt_for_cv_questions(cv_data, position)
        raw_items = self._call_gemini_for_questions(prompt)  # 现在是 list[dict]

        # 如果 LLM 出错，raw_items 可能是空
        if not raw_items:
            print("⚠ LLM 没有返回可用的问题，暂时只保留 Q1/Q2。")
            return [q.to_dict() for q in questions]

        # 只取前三个，作为 Q3–Q5
        max_llm_q = 3
        raw_items = raw_items[:max_llm_q]

        next_id = 3
        for item in raw_items:
            if not isinstance(item, dict):
                continue

            original_main = str(item.get("original_main", "")).strip()
            guided_main = str(item.get("guided_main", "")).strip()

            expected_raw = item.get("expected_time_s", 45)
            try:
                expected_time = float(expected_raw)
            except (TypeError, ValueError):
                expected_time = 45.0

            if not original_main:
                continue

            q = InterviewQuestion(
                id=f"Q{next_id}",
                text=original_main,
                difficulty="medium",
                expected_time_s=expected_time,
                category="cv_based",
                source="cv_based",
                guided_main=guided_main,
            )
            questions.append(q)
            next_id += 1

        return [q.to_dict() for q in questions]



